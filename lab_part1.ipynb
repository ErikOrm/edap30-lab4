{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](blackjack.jpg)",
   "id": "640c43ffeef1fcf7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning: Blackjack and FrozenLake\n",
    "\n",
    "In this lab, we are going to implement and study some simple tabular model-free RL methods. In particular, we will work with BlackJack and Frozenlake.\n",
    "\n",
    "This lab consists of two parts, which are similar in nature. In part one, we will try to develop a learning algorithm to learn a good policy for black jack. And part in two, we will extend this to using SARSA and Q-Learning for Frozen Lake. The difference is that Frozen Lake has much longer episodes, so that it makes sense to learn during the episodes, instead of only after the episode finishes. Note that part two of this lab is in a separate notebook."
   ],
   "id": "1d02481339d88f5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Intro to Reinforcement Learning\n",
    "\n",
    "The field of Reinforcement Learning (RL) is full of various terms and algorithms that all try to do roughly the same thing. That is, to learn an optimal policy in a state-action setting. This can sometimes, be a little confusing, so let us here first with orienting ourselves. We often first distinguish between model-based and model-free RL, where the model in this case is a model of the state transitions. In some problems, we assume that we know the underlying Markov Decision Process (MDP) and its state transitions and rewards. In such cases, we can use value iteration or policy iteration to theoretically calculate the optimal policy.\n",
    "\n",
    "In this lab, even though we theoretically know the underlying model, we will assume that it is unknown to us, and that we will have to learn the underlying policy from experience. In model-free problems, it is often easier to work with state-action values, normally called `Q`, instead of the state value function, often denoted by `V`.\n",
    "\n",
    "There is also a question whether to update the policy during an episode, or wait until the episode has finished. We will use completed episodes in the BlackJack example, and implement SARSA and Q-Learning for the FrozenLake problem."
   ],
   "id": "5fa2260f1a2cbd89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1: Black Jack\n",
    "\n",
    "### Rules of the Game\n",
    "The object of the popular casino card game of blackjack is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. They then immediately wins. If the player does not have a natural, then they can request additional cards, one by one (hits), until they either stops (sticks) or exceeds 21 (goes bust). If the player goes bust, they lose; if they stick, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: they stick on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome—win, lose, or draw—is determined by whose final sum is closer to 21, with the dealer winning draws.\n",
    "\n",
    "### Blackjack in Reinforcement Learning\n",
    "Playing blackjack is naturally formulated as an episodic finite MDP. Each game of blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ( gamma = 1); therefore these terminal rewards are also the returns. The player’s actions are to hit or to stick. The states depend on the player’s cards and the dealer’s showing card. We assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt. As aces can be counted as either 1 or 11, we also need to keep track of the players has a usable ace. Usable in this case means that it is currently counted as 11. If the player would go bust if we count it as 11, we instead count is one and say that they no longer has a usable ace.\n",
    "\n",
    "Hence, we have the following state space: The player's current score (12–21), the dealer’s one showing card (ace–10), and whether or not the player holds a usable ace. This makes for a total of 200 states.\n",
    "\n",
    "The action space, on the other hand is only stick or hit, or 0 or 1."
   ],
   "id": "73c12b79398f89ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Getting acquainted with the environment\n",
    "\n",
    "It is common practice in Reinforcement Learning to define your problem as a 'gym' to keep a consistent format. A gym is a class with a few common elements:\n",
    "- An action space, listing the available actions for the **user**\n",
    "- A state space, listing the format of the problem states\n",
    "- A `reset()` function that restarts and initiates an episode\n",
    "- A `step()` function that takes an action argument and returns the reward and the new state after the action has been performed\n",
    "\n",
    "\n",
    "Gymnasium comes with a set of predefined environments ([https://gymnasium.farama.org/index.html](https://gymnasium.farama.org/index.html)), but you can also create your own.\n",
    "\n",
    "In this case, we have our own gym which is defined in [blackjack.py](blackjack.py).\n",
    "\n",
    "Before we start with the reinforcement learning, let us quickly get acquainted with the blackjack environment."
   ],
   "id": "d8d4f33cc8fab3de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:44:02.885920Z",
     "start_time": "2025-05-18T15:44:02.608694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import some basic packages\n",
    "import plotting\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from blackjack import BlackjackEnv\n",
    "import matplotlib\n",
    "import time\n",
    "from collections import defaultdict\n",
    "matplotlib.style.use('ggplot')"
   ],
   "id": "f631fa24df7710a1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:44:21.716491Z",
     "start_time": "2025-05-18T15:44:21.712416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create the env\n",
    "env = BlackjackEnv()"
   ],
   "id": "10a30eefb8f6d0ca",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Playing around with the Env\n",
    "\n",
    "Here we provide some code defines a simple strategy and tests it against the environment. Before starting the lab make certain you understand how the environment works, and how the observation and actions spaces look like."
   ],
   "id": "ff7360ee88b4bdf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_observation(obs: tuple[int, int, int]):\n",
    "    \"\"\"Prints an observation in a prettier format.\"\"\"\n",
    "    score, dealer_score, usable_ace = obs\n",
    "    print(\n",
    "        \"Player Score: {} (Usable Ace: {}), Dealer Score: {}\".format(\n",
    "          score, usable_ace, dealer_score\n",
    "        )\n",
    "    )\n",
    "\n",
    "def strategy(obs: tuple[int, int, int]) -> int: # return 0 or 1\n",
    "    \"\"\"A strategy takes an observation and returns an action\"\"\"\n",
    "    score, dealer_score, usable_ace = observation\n",
    "\n",
    "    # ➡️ TODO : implement some simple one-line strategy for whether to hit or not ⬅️\n",
    "    return ...\n",
    "\n",
    "# Lastly we put your strategy to the test\n",
    "reward_sum = 0\n",
    "n_iter = 100\n",
    "for i_episode in range(n_iter): # loop over the episodes\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    while not done:\n",
    "        print_observation(observation)\n",
    "        action = strategy(observation)\n",
    "        print(\"Taking action: {}\".format( [\"Stick\", \"Hit\"][action]))\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "    print_observation(observation)\n",
    "    print(\"Game end. Reward: {}\\n\".format(float(reward)))\n",
    "    reward_sum += float(reward)\n",
    "\n",
    "print(\"Your strategy had an average reward of:\", reward_sum / n_iter)"
   ],
   "id": "94f3d6303e799a5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Implement a first Q-Learning algorithm\n",
    "\n",
    "We will use tabular Q-Learning for this problem, meaning that define a value for each state-action pair $q(s,a)$.\n",
    "\n",
    "Q will be stored in the following format:\n",
    "```\n",
    "Q: dict[tuple[int, int, int], npt.NDArray] = defaultdict(lambda: np.zeros(2)),\n",
    "```\n",
    "\n",
    "i.e., the keys of this dict are the state tuples, and the values of this dict are np arrays with two values, one for each action."
   ],
   "id": "ba2c8ba072aed5ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create the Epsilon Greedy Policy\n",
    "\n",
    "First we implement the greedy policy. A policy in this is just a function that takes the state as input and returns the probability to select the different actions. For example, a policy that always stays would return `np.array([1, 0])`.\n",
    "\n",
    "Here, we generate a policy factory, i.e., a function that generates policy functions given the current Q-values and an epsilon."
   ],
   "id": "83930a97434a644b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def epsilon_greedy_policy(\n",
    "        state: tuple[int, int, int],\n",
    "        Q: dict[tuple[int, int, int], npt.NDArray],\n",
    "        epsilon: float,\n",
    "        nA: int,\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Implements an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "\n",
    "    Args:\n",
    "        state (tuple[int, int, int]): The current state of the environment.\n",
    "        Q (dict[tuple[int, int, int], npt.NDArray]): A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA\n",
    "        epsilon (float): The probability to select a random action . float between 0 and 1.\n",
    "        nA (int): Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        npt.NDArray: An array of length 2 with the probabilities to select the different actions.\n",
    "    \"\"\"\n",
    "\n",
    "    return ..."
   ],
   "id": "4e58b0b4f8053d9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The training loop\n",
    "\n",
    "Next, we train our agent using Q-learning. Here you need to implement the full training loop - iteratively generating new episodes and using them to learn your policy function Q. Note that we want $\\varepsilon_k \\rightarrow 0$ as $k\\rightarrow \\infty$.\n",
    "\n",
    "In this part, we do not need to do any TD learning - the episodes are short and never loop back to previous states so we can just update the Q function as the episode is completed."
   ],
   "id": "d47f9fd8238a8855"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T16:02:33.786982Z",
     "start_time": "2025-05-18T16:02:33.783428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mc_control_epsilon_greedy(\n",
    "        env: BlackjackEnv,\n",
    "        num_episodes: int,\n",
    "        epsilon: float = 0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "\n",
    "    Args:\n",
    "        env (BlackjackEnv): OpenAI gym environment.\n",
    "        num_episodes (int): Number of episodes to sample.\n",
    "        epsilon (float): Chance to sample a random action. Float between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        Q (dict[tuple[int, int, int], npt.NDArray]): a dictionary with the Q values.\n",
    "        policy (Callable): a function that takes an observation as an argument and returns\n",
    "        action probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # To estimate the Q values, we keep track of the number of times we have visited\n",
    "    # each state-action-pair, as well as the cumulative reward. We could alternatively,\n",
    "    # store every individual event, but that is much less memory-efficient.\n",
    "\n",
    "    # We leave it up to you define the format of the keys for those.\n",
    "    rewards_sum = defaultdict(float)\n",
    "    rewards_count = defaultdict(float)\n",
    "\n",
    "    # Initialize the state-action values\n",
    "    initial_value = 0\n",
    "    Q: dict[tuple[int, int, int], npt.NDArray] = defaultdict(lambda: np.ones(env.nA) * initial_value)\n",
    "\n",
    "    # ➡️ TODO : use the epsilon_greedy_policy_factory defined above and create a training loop\n",
    "    #    TODO :     that generates new data and gradually updates the Q-function. ⬅️\n",
    "\n",
    "    return Q, policy"
   ],
   "id": "c5811341bcea5259",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "__Questions__ (you might need to run some test through the test code below):\n",
    "- What is the impact of changing `initial_value`? Why would we want to set it higher/lower?\n",
    "- How fast should we let $\\varepsilon$ decay?\n",
    "- How many iterations do we need for this to converge?\n",
    "\n",
    "\n",
    "Let's take it for a spin and see if it works!"
   ],
   "id": "3bc5fa337981d9d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start = time.process_time()\n",
    "\n",
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=5000, epsilon=0.1)\n",
    "\n",
    "end = time.process_time()\n",
    "print(\"\\n\", \"your code ran in\", end - start,\"s\")"
   ],
   "id": "e05bc173c87fffed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Run, test and plot\n",
    "\n",
    "Finally, we run our training and plot the state reward surface. We recommend playing around with num_episodes, epsilon, initial_value as well as the epsilon decay strategy."
   ],
   "id": "4634f2fdc8791227"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "plotting.plot_value_function(V, title=\"Optimal Value Function\")"
   ],
   "id": "3df3ecd99c50d19f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "__Questions__:\n",
    "- Once you have a well working policy, what is the expected reward (before you are dealt your hand) of playing a game of Blackjack? Use your policy to get an estimate.\n",
    "- Often, the plot for \"usable Ace\" look more jagged than the one without - why is that?\n"
   ],
   "id": "6e4a174b5b3d60c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Well done! On to part 2!",
   "id": "d3b4fb8329154d2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
