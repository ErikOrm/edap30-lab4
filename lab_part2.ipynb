{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 2: FrozenLake\n",
    "\n",
    "FrozenLake is a small gridworld problem where you should go from the top left corner to the bottom right corner without hitting a lake. If you hit a lake, you get minus 1 point. If you reach the goal, you get plus one point. There is an option called `is_slippery`, which we will turn on later on in the lab. When the problem is slippery, any action has a 33% probability each to instead go to either of the two perpendicular directions to the one chosen.\n",
    "\n",
    "Direction mapping:\n",
    "0: left\n",
    "1: down\n",
    "2: right\n",
    "3: up\n",
    "\n",
    "The gym is defined in [frozenlake.py](frozenlake.py), but this is in turn a convenience wrapper around the env from gymnasium.\n",
    "\n",
    "The grid world is n by n large, and the squares are defined in an n^2 long vector starting from top left.\n",
    "\n",
    "Statespace: The state space is hence just your current location as an integer $i \\in [0,..,n^2-1]$\n",
    "\n",
    "Actionspace: The action space is an integer between 0 and 3, inclusive."
   ],
   "id": "c6faa1b49bf2780a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "\n",
    "from datasets import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from frozenlake import FrozenLakeEnv\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "env = FrozenLakeEnv(map_size=8, seed=42, is_slippery=False)\n"
   ],
   "id": "f2e7330d8c36872c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Let us first have a look at the environment",
   "id": "383aa5c2fe27c007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"Current state: \", state)\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ],
   "id": "ee9d89be45182c1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Take a few steps",
   "id": "21bd94a22ac889c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# ➡️ TODO : Take a few steps manually and re-render the environment. ⬅️\n",
   "id": "e2582c1ba5892093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Learn a policy: SARSA, Q-Learning, and Episodic\n",
    "\n",
    "Ok, now when we understand how the environment works, let us generate a few policy improvement frameworks. This problem has much longer episodes than BlackJack and so it makes sense to start looking into frameworks that update the policy through bootstrapping, i.e., we no longer only look at the reward for each individual experiment, but instead we update the values of Q based on other values of Q (as in SARSA and Q-Learning), which we can now do during the episode.\n",
    "\n",
    "__Question__: The main difference between SARSA and Q-Learning is that SARSA is \"On-Policy\", whereas Q-Learning is \"Off-Policy\". What does this mean? How do we see it in the formulas? And what impact is it going to have in Frozenlake in particular?\n",
    "\n",
    "Ok, off you go! We have give you a skeleton, but the rest is quite open."
   ],
   "id": "cabecbc37fa6bcbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ➡️ TODO : You should do three things:\n",
    "#    TODO :     - Implement the epsilon_greedy_policy (same as in part 1 so should be quick)\n",
    "#    TODO :     - Implement the training loop and the Q-Learning, SARSA and Episodic improvement steps\n",
    "#    TODO :     - Play around with and try to understand how the different methods work and\n",
    "#    TODO :          how they interplay with the different parameters.\n",
    "#   ⬅️\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(\n",
    "        state_: int,\n",
    "        Q: npt.NDArray,\n",
    "        epsilon: float,\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Takes the state, Q, and epsilon value and returns the probabilities of taking each action.\n",
    "    \"\"\"\n",
    "\n",
    "    # ➡️ TODO : implement the epsilon greedy policy. ⬅️\n",
    "    return ...\n",
    "\n",
    "@dataclass\n",
    "class EpisodeHistory:\n",
    "    \"\"\"A storage container for the data from a single episode\"\"\"\n",
    "    states: npt.NDArray\n",
    "    actions: npt.NDArray\n",
    "    reward: float\n",
    "\n",
    "episodes: list[EpisodeHistory] = []\n",
    "\n",
    "def train_policy(\n",
    "    num_episodes: int,\n",
    "    method: Literal[\"Q-Learning\", \"SARSA\", \"Full-Episode\"],\n",
    "    discount: float,\n",
    "    learning_rate: float,\n",
    "    epsilon: float,\n",
    "    initial_value: float,\n",
    ") -> npt.NDArray:\n",
    "\n",
    "    \"\"\"\n",
    "    Takes some useful input (add/remove/rename as you like) and returns the policy state-action values Q.\n",
    "    \"\"\"\n",
    "\n",
    "    # Here, the statespace is simpler, so we instead just save the Q values in an (n^2 x 4) matrix\n",
    "    # with the states in the first dimension and the actions in the second.\n",
    "    Q = np.ones((64, 4)) * initial_value\n",
    "    Q[-1, :] = np.zeros((1, 4)) # There is no value of being in the final state, only the reward\n",
    "\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        state_history: list[int] = []    # used for plotting\n",
    "        action_history: list[int] = []   # used for plotting\n",
    "        reward = 0\n",
    "\n",
    "\n",
    "        # ➡️ TODO : Create training data and update the Q values. ⬅️\n",
    "        # ➡️ TODO : If you want the plots below to work, also update the stat history and action history ⬅️\n",
    "\n",
    "\n",
    "        episodes.append(\n",
    "            EpisodeHistory(\n",
    "                states=np.array(state_history),\n",
    "                actions=np.array(action_history),\n",
    "                reward=reward,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "# ➡️ TODO : run it with some different values for the hyperparameters. ⬅️\n",
    "Q = train_policy(\n",
    "    num_episodes=...,\n",
    "    method=...,\n",
    "    discount=...,\n",
    "    learning_rate=...,\n",
    "    epsilon=...,\n",
    "    initial_value=...,\n",
    ")\n"
   ],
   "id": "7adb2ed3888aa15a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Let us plot the state values function as we did in Part 1",
   "id": "90223b3b554f9c7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.plot_value_function(Q)",
   "id": "9b1c237531177f27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Average reward\n",
    "\n",
    "We can also plot the average reward every 100 episodes. The code for that is below."
   ],
   "id": "485d822c179a9e1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_average_performance(\n",
    "        episodes: list[EpisodeHistory] | list[list[EpisodeHistory]],\n",
    "        names: list[str] = [],\n",
    "):\n",
    "    if len(episodes) == 0:\n",
    "        print(\"Nothing to plot\")\n",
    "        return\n",
    "\n",
    "    if isinstance(episodes[0], EpisodeHistory):\n",
    "        episodes = [episodes]\n",
    "\n",
    "    names = names + [f\"exp {i}\" for i in range(len(episodes) - len(names))]\n",
    "\n",
    "    for eps, name in zip(episodes, names):\n",
    "        print(\"Plotting\", name)\n",
    "        rewards = np.array([e.reward for e in eps])\n",
    "        avg_rewards = rewards[:rewards.shape[0] // 100 * 100].reshape(-1, 100).mean(axis=1)\n",
    "        plt.plot(avg_rewards, label=name)\n",
    "        plt.xlabel(\"Episodes (x100)\")\n",
    "        plt.ylabel(\"Average reward\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_average_performance(episodes=episodes)"
   ],
   "id": "5de130f4a0b0c6d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tests, conclusions and analysis\n",
    "\n",
    "With the three methods tested and implemented, what did we see?\n",
    "\n",
    "- Which methods worked better, which worked worse?\n",
    "- What parameters seemed important? Any ideas why?\n",
    "\n",
    "If you want to, you can up to where we define the gym and set `is_slippery`to `True`, to see how that affects the results."
   ],
   "id": "73cb7a75a83871ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    "In this lab we are still working with Tabular RL. In practice, we quite often cannot enumerate the search space, and in those cases we need a model to approximate Q. n most cases, this leads to Deep Reinforcement Learning, which would be the natural next step."
   ],
   "id": "d70cc19ebd12832a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d0392436e593f8cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
